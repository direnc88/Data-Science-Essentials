{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](Header__0000_10.png)\n",
    "___\n",
    "# Chapter 10 -  Web Scraping with Beautiful Soup\n",
    "## Segment 4 - Web scraping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# we'll learn:\n",
    "# - scraping a webpage\n",
    "# - saving the results to a file. \n",
    "#\n",
    "#\n",
    "#\n",
    "#\n",
    "#\n",
    "#\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import urllib\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "bs4.BeautifulSoup"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# our variable is r. Using urlib and urlopen to scrape our chosen weblink/url. \n",
    "r = urllib.urlopen('https://analytics.usa.gov').read()\n",
    "#creating a beautiful soup object calling it soup. and a beautiful soup constructor passing in r. \n",
    "soup = BeautifulSoup(r, \"lxml\")\n",
    "\n",
    "#checking type of beautiful soup object. \n",
    "type(soup)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scraping a webpage and saving your results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<!DOCTYPE html>\n",
      "<html lang=\"en\">\n",
      " <!-- Initalize title and data source variables -->\n",
      " <head>\n",
      "  <!--\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#printing out the first 100 elements with prettify. \n",
    "print soup.prettify()[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/\n",
      "#explanation\n",
      "https://analytics.usa.gov/data/\n",
      "data/\n",
      "#top-pages-realtime\n",
      "#top-pages-7-days\n",
      "#top-pages-30-days\n",
      "https://analytics.usa.gov/data/live/all-pages-realtime.csv\n",
      "https://analytics.usa.gov/data/live/top-domains-30-days.csv\n",
      "https://www.digitalgov.gov/services/dap/\n",
      "https://www.digitalgov.gov/services/dap/common-questions-about-dap-faq/#part-4\n",
      "https://support.google.com/analytics/answer/2763052?hl=en\n",
      "https://analytics.usa.gov/data/live/second-level-domains.csv\n",
      "https://analytics.usa.gov/data/live/sites.csv\n",
      "mailto:DAP@support.digitalgov.gov\n",
      "https://github.com/GSA/analytics.usa.gov\n",
      "https://github.com/18F/analytics-reporter\n",
      "https://github.com/GSA/analytics.usa.gov/issues\n",
      "mailto:DAP@support.digitalgov.gov\n",
      "https://analytics.usa.gov/data/\n"
     ]
    }
   ],
   "source": [
    "#finding all of the a tags and retreiving all of the href links (weblinks) from the a tags. \n",
    "for link in soup.find_all('a'): print(link.get('href'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<a href=\"https://analytics.usa.gov/data/\">Data</a>\n",
      "<a href=\"https://analytics.usa.gov/data/live/all-pages-realtime.csv\">Download the full dataset.</a>\n",
      "<a href=\"https://analytics.usa.gov/data/live/top-domains-30-days.csv\">Download the full dataset.</a>\n",
      "<a class=\"external-link\" href=\"https://www.digitalgov.gov/services/dap/\">Digital Analytics Program</a>\n",
      "<a class=\"external-link\" href=\"https://www.digitalgov.gov/services/dap/common-questions-about-dap-faq/#part-4\">does not track individuals</a>\n",
      "<a class=\"external-link\" href=\"https://support.google.com/analytics/answer/2763052?hl=en\">anonymizes the IP addresses</a>\n",
      "<a class=\"external-link\" href=\"https://analytics.usa.gov/data/live/second-level-domains.csv\">400 executive branch government domains</a>\n",
      "<a class=\"external-link\" href=\"https://analytics.usa.gov/data/live/sites.csv\">about 5000 total websites</a>\n",
      "<a class=\"external-link\" href=\"https://github.com/GSA/analytics.usa.gov\">code for this website</a>\n",
      "<a class=\"external-link\" href=\"https://github.com/18F/analytics-reporter\">code behind the data collection</a>\n",
      "<a class=\"external-link\" href=\"https://github.com/GSA/analytics.usa.gov/issues\">open an issue on GitHub</a>\n",
      "<a href=\"https://analytics.usa.gov/data/\">download the data here.</a>\n"
     ]
    }
   ],
   "source": [
    "#passing through the soup object with a for loop to find all of the a tags with an attribute of href. \n",
    "#of all the tags that are returned we want the regular expression to match against them to return only those with \n",
    "# 'http' and then print those out. \n",
    "# all a tags with an attribute of href and also have an http match within them. \n",
    "#\n",
    "for link in soup.findAll('a', attrs={'href': re.compile(\"^http\")}): print link"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<a href=\"https://analytics.usa.gov/data/\">Data</a>\n",
      "<a href=\"https://analytics.usa.gov/data/live/all-pages-realtime.csv\">Download the full dataset.</a>\n",
      "<a href=\"https://analytics.usa.gov/data/live/top-domains-30-days.csv\">Download the full dataset.</a>\n",
      "<a class=\"external-link\" href=\"https://www.digitalgov.gov/services/dap/\">Digital Analytics Program</a>\n",
      "<a class=\"external-link\" href=\"https://www.digitalgov.gov/services/dap/common-questions-about-dap-faq/#part-4\">does not track individuals</a>\n",
      "<a class=\"external-link\" href=\"https://support.google.com/analytics/answer/2763052?hl=en\">anonymizes the IP addresses</a>\n",
      "<a class=\"external-link\" href=\"https://analytics.usa.gov/data/live/second-level-domains.csv\">400 executive branch government domains</a>\n",
      "<a class=\"external-link\" href=\"https://analytics.usa.gov/data/live/sites.csv\">about 5000 total websites</a>\n",
      "<a class=\"external-link\" href=\"https://github.com/GSA/analytics.usa.gov\">code for this website</a>\n",
      "<a class=\"external-link\" href=\"https://github.com/18F/analytics-reporter\">code behind the data collection</a>\n",
      "<a class=\"external-link\" href=\"https://github.com/GSA/analytics.usa.gov/issues\">open an issue on GitHub</a>\n",
      "<a href=\"https://analytics.usa.gov/data/\">download the data here.</a>\n"
     ]
    }
   ],
   "source": [
    "#saving to an external file. creating a file and naming it. \n",
    "#wb tells python to write into this file. \n",
    "#for each link that is found we need to turn it into a string before printing it out. \n",
    "file = open('parsed_data.txt', 'wb')\n",
    "for link in soup.findAll('a', attrs={'href': re.compile(\"^http\")}):\n",
    "    soup_link = str(link)\n",
    "    print soup_link\n",
    "    #writing it into the file. \n",
    "    file.write(soup_link)\n",
    "file.flush()\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "u'C:\\\\Users\\\\Lillian Pierson\\\\Desktop\\\\Exercise Files\\\\Ch10\\\\10_04'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#to find out where the file is saved. \n",
    "#this file will have each of the links from the webpage with a few stray characters. \n",
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
