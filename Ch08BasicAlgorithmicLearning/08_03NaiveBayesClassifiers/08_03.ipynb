{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](Header__0002_8.png)\n",
    "___\n",
    "# Chapter 8 - Basic Algorithmic Learning\n",
    "## Segment 3 - Naive Bayes Classifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# We'll be building a spam filter.  \n",
    "#\n",
    "# Naive Bayes is a machine learning method you can use to predict the likelihood that an event will occur given \n",
    "# evidence that's present in your data. \n",
    "#\n",
    "# Conditional probability   \n",
    "# P(BIA) = P(A and B)/ P(A)\n",
    "#\n",
    "# 3 types of Naive Bayes Models:\n",
    "#   1 - Multinomial - Good for when your features (categorical or continuous) describe discrete frequency counts\n",
    "# (Ex word counts)\n",
    "#  2 - Bernoulli - good for making predictions form binary features. \n",
    "#  3 - Gaussian - good for making predictions from normally distributed features. \n",
    "#\n",
    "# Use Cases:\n",
    "# - Spam detection\n",
    "# - Customer Classification\n",
    "# - Credit risk prediction\n",
    "# - health risk prediction\n",
    "#\n",
    "# Assumptions:\n",
    "# - Predictions are independent of each other. \n",
    "# - A priori assumption: this is an assumption that the past conditions still hold true. When we make predictions from \n",
    "# historical values, we will get incorrect results if present circumstances have changed. \n",
    "# - All regression models maintain an a priori assumpiton as well. \n",
    "#\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import urllib\n",
    "\n",
    "import sklearn\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naive Bayes\n",
    "### Using Naive Bayes to predict spam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[   0.       0.64     0.64     0.       0.32     0.       0.       0.       0.\n",
      "    0.       0.       0.64     0.       0.       0.       0.32     0.\n",
      "    1.29     1.93     0.       0.96     0.       0.       0.       0.       0.\n",
      "    0.       0.       0.       0.       0.       0.       0.       0.       0.\n",
      "    0.       0.       0.       0.       0.       0.       0.       0.       0.\n",
      "    0.       0.       0.       0.       0.       0.       0.       0.778\n",
      "    0.       0.       3.756   61.     278.       1.   ]\n"
     ]
    }
   ],
   "source": [
    "# The url is where the spam dataset comes from. It comes from the university of California Irvine\n",
    "# calling the dataset raw_data. need to use the url open function to bring in the data. \n",
    "url = \"https://archive.ics.uci.edu/ml/machine-learning-databases/spambase/spambase.data\"\n",
    "raw_data = urllib.urlopen(url)\n",
    "#using the npy loadtxt function to load the dataset in as csv. labeling the delimiter ,\n",
    "dataset = np.loadtxt(raw_data, delimiter=\",\")\n",
    "print dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#now we need to isolate the predictor variables in the dataset. These 48 features describe word frequency counts. \n",
    "#calling the dataset of predictive features x. \n",
    "X = dataset[:,0:48]\n",
    "\n",
    "#isolating the target variable. spam will get a label of 1 and not spam will get a label of 0. \n",
    "#target variable is y. \n",
    "y = dataset[:, -1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#labeling our test and training datasets.  \n",
    "#test size is .33 so 33% is going to be used for testing. \n",
    "#random state is basically the seed. \n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.33, random_state=17)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BernoulliNB(alpha=1.0, binarize=True, class_prior=None, fit_prior=True)\n",
      "0.855826201448\n"
     ]
    }
   ],
   "source": [
    "#the dataset is filled with continuous variables that describe frequency counts words. \n",
    "#we'll be combining bernouliNB with binning to convert the frequency counts to values. \n",
    "#we also pass in our xtrain and ytrain\n",
    "BernNB = BernoulliNB(binarize=True)\n",
    "BernNB.fit(X_train, y_train)\n",
    "print(BernNB)\n",
    "\n",
    "#generating some predictive labels from our model. \n",
    "y_expect = y_test\n",
    "y_pred = BernNB.predict(X_test)\n",
    "\n",
    "#assessing the accuracy of the model. accuracy score of 85 for the model. \n",
    "print accuracy_score(y_expect, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True)\n",
      "0.873601053325\n"
     ]
    }
   ],
   "source": [
    "#calling fit method on multinomial Niave Bayes Model. \n",
    "\n",
    "MultiNB = MultinomialNB()\n",
    "\n",
    "MultiNB.fit(X_train, y_train)\n",
    "print(MultiNB)\n",
    "\n",
    "#printing in accuracy again. 87% so it's a bit better. \n",
    "y_pred = MultiNB.predict(X_test)\n",
    "print accuracy_score(y_expect, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GaussianNB()\n",
      "0.813034891376\n"
     ]
    }
   ],
   "source": [
    "GausNB = GaussianNB()\n",
    "GausNB.fit(X_train, y_train)\n",
    "print(GausNB)\n",
    "\n",
    "y_pred = GausNB.predict(X_test)\n",
    "#81% accuracy\n",
    "print accuracy_score(y_expect, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BernoulliNB(alpha=1.0, binarize=0.1, class_prior=None, fit_prior=True)\n",
      "0.895325872284\n"
     ]
    }
   ],
   "source": [
    "#through trail and error we are able to optimize the accuracy of our model. \n",
    "#binarize is different. \n",
    "BernNB = BernoulliNB(binarize=0.1)\n",
    "BernNB.fit(X_train, y_train)\n",
    "print(BernNB)\n",
    "\n",
    "y_expect = y_test\n",
    "y_pred = BernNB.predict(X_test)\n",
    "print accuracy_score(y_expect, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
